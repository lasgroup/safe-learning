# @package _global_
defaults:
  - override /environment: rccar_real
  - override /agent: sbsrl
  - override /agent/penalizer: multi_lagrangian
  - _self_

environment:
  action_delay: 1
  observation_delay: 0
  sliding_window: 5
  dt: 0.03333333
  sample_init_pose: true

training:
  num_envs: 4096
  num_eval_envs: 4096
  num_timesteps: 50000000
  safe: true
  episode_length: 250
  num_evals: 20
  train_domain_randomization: false
  eval_domain_randomization: false
  safety_budget: 5.0
  num_eval_episodes: 1
  wandb_id: 6a3fze

agent:
  activation: swish
  max_replay_size: 1048576
  policy_hidden_layer_sizes: [64, 64] #policy_hidden_layer_sizes: [256, 256, 256]
  value_hidden_layer_sizes: [512, 512]
  batch_size: 256
  critic_grad_updates_per_step: 10
  model_grad_updates_per_step: 50
  num_model_rollouts: 50000
  normalize_budget: true
  use_mean_critic: true
  offline: true
  save_sooper_backup: true
  flip_uncertainty_constraint: true
  uncertainty_constraint: true
  uncertainty_epsilon: 30
  cost_scaling: 1
  reward_scaling: 1
  model_to_real_data_ratio: 0.25
  reward_pessimism: 0.
  penalizer:
    initial_multiplier_uncertainty: 0.01
    initial_multiplier_cost: 0.01
    learning_rate: 1e-4



